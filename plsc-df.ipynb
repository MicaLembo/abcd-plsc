{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the main directory and target directory paths\n",
    "data_dir = \"./dset\"\n",
    "deriv_dir = \"./derivatives\"\n",
    "os.makedirs(deriv_dir, exist_ok=True)\n",
    "demo_dir = op.join(deriv_dir, \"demo\")\n",
    "os.makedirs(demo_dir, exist_ok=True)\n",
    "rsfc_dir = op.join(deriv_dir, \"rsfc\")\n",
    "os.makedirs(rsfc_dir, exist_ok=True)\n",
    "hisp_dir = op.join(deriv_dir, \"hispanic\")\n",
    "os.makedirs(hisp_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer the Data\n",
    "#### Select certain ABCD variables from CSVs to be copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that don't need to be controlled for age\n",
    "# Define the lists of filenames and variables to extract\n",
    "file_lists = {\n",
    "    \"abcd\": {\n",
    "        \"abcd_y_lt\": [\"interview_age\", \"rel_family_id\", \"site_id_l\"],\n",
    "        \"abcd_p_demo\": [\n",
    "            \"demo_sex_v2\",\n",
    "            \"demo_ethn_v2\",\n",
    "            \"demo_race_a_*\",\n",
    "            \"demo_prnt_age_v2\",\n",
    "            \"demo_prnt_gender_id_v2\",\n",
    "            \"demo_prnt_ethn_v2\",\n",
    "            \"demo_prnt_race_a_*\",\n",
    "            \"demo_prnt_ed_v2_2yr_l\",\n",
    "            \"demo_prtnr_ed_v2_2yr_l\",\n",
    "            \"demo_comb_income_v2\",\n",
    "            \"demo_prnt_income_v2_l\",\n",
    "            \"demo_origin_v2\",\n",
    "            \"demo_biomother_v2\",\n",
    "            \"demo_biofather_v2\",\n",
    "            \"demo_matgrandm_v2\",\n",
    "            \"demo_matgrandf_v2\",\n",
    "            \"demo_patgrandm_v2\",\n",
    "            \"demo_patgrandf_v2\",\n",
    "        ],\n",
    "    },\n",
    "    \"led_l\": {\n",
    "        \"led_l_coi\": [\"reshist_addr1_coi_r_coi_nat\"],\n",
    "        \"led_l_nbhsoc\": [\"reshist_addr1_nanda_disadv_fac\"],\n",
    "        \"led_l_gi\": [\"reshist_addr1_gstat_h_queen\"],\n",
    "    },\n",
    "    \"ph_y\": {\n",
    "        \"ph_y_anthro\": [\"anthroheightcalc\", \"anthroweightcalc\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Function to recursively find files in directories\n",
    "def find_files(directory):\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            yield os.path.join(dirpath, filename)\n",
    "\n",
    "\n",
    "# Iterate through each list and process corresponding files\n",
    "for category, files_and_vars in file_lists.items():\n",
    "    for filename, vars_to_extract in files_and_vars.items():\n",
    "        # Search for the file in all subdirectories of data_directory\n",
    "        found_file = False\n",
    "        for file_path in find_files(data_dir):\n",
    "            if filename + \".csv\" in file_path:\n",
    "                # Read the CSV file and extract specified columns\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Ordered dictionary to maintain the order of columns\n",
    "                columns_to_keep = OrderedDict()\n",
    "                columns_to_keep[\"src_subject_id\"] = True\n",
    "                columns_to_keep[\"eventname\"] = True\n",
    "\n",
    "                for col in vars_to_extract:\n",
    "                    if \"*\" in col:\n",
    "                        # Use regular expression to match the pattern\n",
    "                        regex_pattern = col.replace(\"*\", \".*\")\n",
    "                        matched_columns = list(df.filter(regex=regex_pattern).columns)\n",
    "                        for matched_col in matched_columns:\n",
    "                            columns_to_keep[matched_col] = True\n",
    "                    else:\n",
    "                        columns_to_keep[col] = True\n",
    "\n",
    "                # Create a subset of the dataframe with the desired columns\n",
    "                df_subset = df[list(columns_to_keep.keys())]\n",
    "\n",
    "                # Construct the destination path and filename\n",
    "                output_filename = f\"{filename}_subset.csv\"\n",
    "                output_path = os.path.join(demo_dir, output_filename)\n",
    "\n",
    "                # Save the subset dataframe to the target directory\n",
    "                df_subset.to_csv(output_path, index=False)\n",
    "\n",
    "                print(f\"Saved {output_filename} to {demo_dir}\")\n",
    "\n",
    "                found_file = True\n",
    "                break\n",
    "\n",
    "        if not found_file:\n",
    "            print(f\"File {filename}.csv not found in {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify certain years to be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables that need to be controlled for specific years\n",
    "\n",
    "# Define the lists of filenames and shared eventname\n",
    "file_lists = {\n",
    "    \"ce_y\": {\n",
    "        \"ce_y_meim\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"meim_ss_exp\",\n",
    "                \"meim_ss_com\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_y_via\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"via_ss_hc\",\n",
    "                \"via_ss_amer\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_y_macv\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"macv_y_ss_fs\",\n",
    "                \"macv_y_ss_fo\",\n",
    "                \"macv_y_ss_fr\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_y_dm\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"dim_y_ss_mean\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"ce_p\": {\n",
    "        \"ce_p_meim\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"meim_p_ss_exp\",\n",
    "                \"meim_p_ss_com\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_p_via\": {\n",
    "            \"eventname\": \"3_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"via_p_ss_hc\",\n",
    "                \"via_p_ss_amer\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_p_macv\": {\n",
    "            \"eventname\": \"2_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"macv_p_ss_fs\",\n",
    "                \"macv_p_ss_fo\",\n",
    "                \"macv_p_ss_fr\",\n",
    "            ],\n",
    "        },\n",
    "        \"ce_p_comc\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"comc_ss_cohesion_p\",\n",
    "                \"comc_ss_control_p\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"ph_y\": {\n",
    "        \"ph_y_yrb\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"physical_activity1_y\",\n",
    "            ],\n",
    "        },\n",
    "        \"ph_y_resp\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"resp_wheeze_yn_y\",\n",
    "                \"resp_pmcough_yn_y\",\n",
    "                \"resp_diagnosis_yn_y\",\n",
    "                \"resp_bronch_yn_y\",\n",
    "            ],\n",
    "        },\n",
    "        \"ph_y_mctq\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"mctq_sdweek_calc\",\n",
    "                \"mctq_msfsc_calc\",\n",
    "            ],\n",
    "        },\n",
    "        \"ph_y_bp\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"blood_pressure_sys_mean\",\n",
    "                \"blood_pressure_dia_mean\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"mh_p\": {\n",
    "        \"mh_p_cbcl\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"cbcl_scr_syn_internal_t\",\n",
    "                \"cbcl_scr_syn_external_t\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "    \"mri_y\": {\n",
    "        \"mri_y_rsfmr_cor_gp_gp\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"copy_entire_file\": True,\n",
    "        },\n",
    "        \"mri_y_adm_info\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"mri_info_manufacturer\",\n",
    "            ],\n",
    "        },\n",
    "        \"mri_y_qc_motion\": {\n",
    "            \"eventname\": \"4_year_follow_up_y_arm_1\",\n",
    "            \"columns_to_extract\": [\n",
    "                \"src_subject_id\",\n",
    "                \"eventname\",\n",
    "                \"rsfmri_meanmotion\",\n",
    "            ],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# Function to recursively find files in directories\n",
    "def find_files(directory):\n",
    "    for dirpath, _, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            yield os.path.join(dirpath, filename)\n",
    "\n",
    "\n",
    "# Iterate through each list and process corresponding files\n",
    "for category, files_and_info in file_lists.items():\n",
    "    for filename, info in files_and_info.items():\n",
    "        eventname = info[\"eventname\"]\n",
    "        copy_entire_file = info.get(\"copy_entire_file\", False)\n",
    "        columns_to_extract = info.get(\"columns_to_extract\", [])\n",
    "\n",
    "        # Search for the file in all subdirectories of data_directory\n",
    "        found_file = False\n",
    "        for file_path in find_files(data_dir):\n",
    "            if filename + \".csv\" in file_path:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                # Filter rows by the shared eventname\n",
    "                df_filtered = df[df[\"eventname\"] == eventname]\n",
    "\n",
    "                if not copy_entire_file and columns_to_extract:\n",
    "                    # Create subset dataframe with desired columns\n",
    "                    df_filtered = df_filtered[columns_to_extract]\n",
    "\n",
    "                # Construct the destination path and filename\n",
    "                output_filename = f\"{filename}_subset.csv\"\n",
    "                output_path = os.path.join(demo_dir, output_filename)\n",
    "\n",
    "                # Save the filtered dataframe to the target directory\n",
    "                df_filtered.to_csv(output_path, index=False)\n",
    "\n",
    "                print(f\"Saved {output_filename} to {demo_dir}\")\n",
    "\n",
    "                found_file = True\n",
    "                break\n",
    "\n",
    "        if not found_file:\n",
    "            print(f\"File {filename}.csv not found in {demo_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the demographic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the demo files\n",
    "file_names = [\n",
    "    \"abcd_y_lt_subset.csv\",\n",
    "    \"abcd_p_demo_subset.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "# Loop over each file name\n",
    "for file_name in file_names:\n",
    "    # Define the path to the CSV file\n",
    "    file_path = op.join(demo_dir, file_name)\n",
    "\n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Forward fill within each 'src_subject_id' group\n",
    "    df = df.set_index(\"src_subject_id\").groupby(\"src_subject_id\").ffill().reset_index()\n",
    "\n",
    "    # Filter rows where 'eventname' is '4_year_follow_up_y_arm_1'\n",
    "    df = df[df[\"eventname\"] == \"4_year_follow_up_y_arm_1\"]\n",
    "\n",
    "    # Additional filtering for the 'abcd_p_demo_subset.csv' file\n",
    "    if file_name == \"abcd_p_demo_subset.csv\":\n",
    "        df = df[df[\"demo_ethn_v2\"] == 1]\n",
    "\n",
    "    # Reset index and ensure 'src_subject_id' is preserved\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df.columns)\n",
    "    print(df)\n",
    "\n",
    "    # Save the processed DataFrame\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns related to 'race'\n",
    "race_cols = [col for col in df.columns if \"race\" in col]\n",
    "\n",
    "# Create the new column based on the presence of any value that is not 0 or NaN\n",
    "df[\"demo_prnt_race\"] = df[race_cols].apply(\n",
    "    lambda x: (\n",
    "        x.index[(x != 0) & (~x.isna())].tolist()[0]\n",
    "        if not ((x == 0) | (x.isna())).all()\n",
    "        else None\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Drop columns that start with 'demo_race_a_p___'\n",
    "df = df.loc[:, ~df.columns.str.startswith(\"demo_race_a_p___\")]\n",
    "df = df.loc[:, ~df.columns.str.startswith(\"demo_prnt_race_a\")]\n",
    "\n",
    "\n",
    "# Define a custom aggregation function\n",
    "def aggregate_column(series):\n",
    "    # For columns with multiple values, return a list of values, else the first value\n",
    "    if series.nunique() > 1:\n",
    "        return \", \".join(series.dropna().astype(str))\n",
    "    return series.iloc[0] if not series.empty else pd.NA\n",
    "\n",
    "\n",
    "# Merge rows based on 'src_subject_id', aggregating values in other columns\n",
    "df_merged = df.groupby(\"src_subject_id\").agg(aggregate_column).reset_index()\n",
    "\n",
    "print(df_merged)\n",
    "\n",
    "# Save the DataFrame with the new column to a new CSV file\n",
    "df_merged.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of columns to keep\n",
    "columns_to_keep = [\n",
    "    \"src_subject_id\",\n",
    "    \"demo_sex_v2\",\n",
    "    \"demo_ethn_v2\",\n",
    "    \"demo_ethn2_v2\",\n",
    "    \"demo_prnt_age_v2\",\n",
    "    \"demo_prnt_gender_id_v2\",\n",
    "    \"demo_prnt_ethn_v2\", \n",
    "    \"demo_prnt_ethn2_v2\",\n",
    "    \"demo_prnt_ed_v2_2yr_l\",\n",
    "    \"demo_prtnr_ed_v2_2yr_l\",\n",
    "    \"demo_comb_income_v2\",\n",
    "    \"demo_prnt_income_v2_l\",\n",
    "    \"demo_origin_v2\",\n",
    "    \"demo_biomother_v2\",\n",
    "    \"demo_biofather_v2\",\n",
    "    \"demo_matgrandm_v2\",\n",
    "    \"demo_matgrandf_v2\",\n",
    "    \"demo_patgrandm_v2\",\n",
    "    \"demo_patgrandf_v2\",\n",
    "    'demo_prnt_race',\n",
    "]\n",
    "\n",
    "# Ensure that columns in the DataFrame are in the columns_to_keep list\n",
    "columns_to_keep = [col for col in columns_to_keep if col in df_merged.columns]\n",
    "\n",
    "# Filter the DataFrame to keep only the desired columns\n",
    "df_filtered = df_merged[columns_to_keep]\n",
    "\n",
    "# Optionally, check the columns to be removed\n",
    "columns_to_remove = [col for col in df.columns if col not in columns_to_keep]\n",
    "\n",
    "# Print columns to remove for verification\n",
    "print(\"Columns to remove:\", columns_to_remove)\n",
    "\n",
    "# Print the DataFrame with only the columns to keep\n",
    "print(df_filtered)\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "df_filtered.to_csv(op.join(demo_dir, \"abcd_p_demo_subset.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### calculate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output file\n",
    "output_file = op.join(demo_dir, \"excluded-ppts.txt\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    # Loop through each file in the directory\n",
    "    for filename in os.listdir(demo_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(demo_dir, filename)\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure the DataFrame is not empty\n",
    "            if df.empty:\n",
    "                file.write(f\"File: {filename} is empty.\\n\")\n",
    "                continue\n",
    "\n",
    "            # Calculate the percentage of missing data for each column\n",
    "            missing_data_per_column = df.isna().mean() * 100\n",
    "\n",
    "            # Calculate the percentage of rows with any NaN values\n",
    "            rows_with_nan = df.isna().any(axis=1).mean() * 100\n",
    "\n",
    "            # Find columns with NaN values\n",
    "            columns_with_nan = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "            if columns_with_nan:\n",
    "                file.write(f\"File: {filename}\\n\")\n",
    "                file.write(\"Percentage of missing data per column:\\n\")\n",
    "                for column in columns_with_nan:\n",
    "                    percentage_missing = df[column].isna().mean() * 100\n",
    "                    if percentage_missing > 5:\n",
    "                        file.write(f\"{column}: {percentage_missing:.2f}% *\\n\")\n",
    "                if rows_with_nan > 5:\n",
    "                    file.write(\n",
    "                        f\"Percentage of rows with any NaN values: {rows_with_nan:.2f}% *\\n\"\n",
    "                    )\n",
    "\n",
    "                # Remove rows with NaN values\n",
    "                df_cleaned = df.dropna()\n",
    "                non_empty_rows_left = len(df_cleaned)\n",
    "                file.write(\n",
    "                    f\"Total number of non-empty rows left: {non_empty_rows_left}\\n\"\n",
    "                )\n",
    "                file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSFC\n",
    "#### only keep subjects scanned at year 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the main CSV file with the src_subject_id column\n",
    "rsfc_df = op.join(demo_dir, \"mri_y_rsfmr_cor_gp_gp_subset.csv\")\n",
    "\n",
    "# Read the main CSV file to get the list of src_subject_id values\n",
    "main_df = pd.read_csv(rsfc_df)\n",
    "valid_subject_ids = main_df[\"src_subject_id\"].unique()\n",
    "\n",
    "\n",
    "# First, filter and save all CSV files\n",
    "for filename in os.listdir(demo_dir):\n",
    "    if filename.endswith(\".csv\") and filename != \"mri_y_rsfmr_cor_gp_gp_subset.csv\":\n",
    "        file_path = os.path.join(demo_dir, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Filter the DataFrame to keep only rows with valid src_subject_id values\n",
    "        df_filtered = df[df[\"src_subject_id\"].isin(valid_subject_ids)]\n",
    "\n",
    "        # Save the filtered DataFrame to a new CSV file\n",
    "        output_csv_file = os.path.join(rsfc_dir, filename)\n",
    "        df_filtered.to_csv(output_csv_file, index=False)\n",
    "\n",
    "\n",
    "# Copy the rsfc_df to the rsfc_dir\n",
    "shutil.copy(rsfc_df, rsfc_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter for only Hispanic/Latino/Latina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the main CSV file with the src_subject_id column\n",
    "hisp_df = op.join(rsfc_dir, \"abcd_p_demo_subset.csv\")\n",
    "\n",
    "# Read the main CSV file to get the list of src_subject_id values\n",
    "main_df = pd.read_csv(hisp_df)\n",
    "valid_subject_ids = main_df[\"src_subject_id\"].unique()\n",
    "\n",
    "\n",
    "# First, filter and save all CSV files\n",
    "for filename in os.listdir(rsfc_dir):\n",
    "    if filename.endswith(\".csv\") and filename != \"abcd_p_demo_subset.csv\":\n",
    "        file_path = os.path.join(demo_dir, filename)\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Filter the DataFrame to keep only rows with valid src_subject_id values\n",
    "        df_filtered = df[df[\"src_subject_id\"].isin(valid_subject_ids)]\n",
    "\n",
    "        # Save the filtered DataFrame to a new CSV file\n",
    "        output_csv_file = os.path.join(hisp_dir, filename)\n",
    "        df_filtered.to_csv(output_csv_file, index=False)\n",
    "\n",
    "\n",
    "# Copy the rsfc_df to the rsfc_dir\n",
    "shutil.copy(hisp_df, hisp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, calculate the percentage of missing data for each filtered CSV\n",
    "output_file = os.path.join(hisp_dir, \"excluded_data_summary.txt\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    for filename in os.listdir(hisp_dir):\n",
    "        if filename.endswith(\".csv\") and filename != \"excluded_data_summary.txt\":\n",
    "            file_path = os.path.join(hisp_dir, filename)\n",
    "            # Read the filtered CSV file into a DataFrame\n",
    "            df_filtered = pd.read_csv(file_path)\n",
    "\n",
    "            # Calculate the percentage of missing data for each column\n",
    "            missing_data_per_column = df_filtered.isna().mean() * 100\n",
    "\n",
    "            # Calculate the percentage of rows with any NaN values\n",
    "            rows_with_nan = df_filtered.isna().any(axis=1).mean() * 100\n",
    "\n",
    "            # Find columns with NaN values\n",
    "            columns_with_nan = df_filtered.columns[df_filtered.isna().any()].tolist()\n",
    "\n",
    "            if columns_with_nan:\n",
    "                file.write(f\"File: {filename}\\n\")\n",
    "                file.write(\"Percentage of missing data per column:\\n\")\n",
    "                for column in columns_with_nan:\n",
    "                    percentage_missing = df_filtered[column].isna().mean() * 100\n",
    "                    if percentage_missing > 5:\n",
    "                        file.write(f\"{column}: {percentage_missing:.2f}% *\\n\")\n",
    "                    else:\n",
    "                        file.write(f\"{column}: {percentage_missing:.2f}%\\n\")\n",
    "                if rows_with_nan > 5:\n",
    "                    file.write(\n",
    "                        f\"Percentage of subjects missing data: {rows_with_nan:.2f}% *\\n\"\n",
    "                    )\n",
    "                else:\n",
    "                    file.write(\n",
    "                        f\"Percentage of subjects missing data: {rows_with_nan:.2f}%\\n\"\n",
    "                    )\n",
    "\n",
    "                non_empty_rows_left = len(df_filtered.dropna())\n",
    "                file.write(\n",
    "                    f\"Total number of subjects left: {non_empty_rows_left}\\n\"\n",
    "                )\n",
    "                file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames for each category\n",
    "sociocult_df = pd.DataFrame()\n",
    "covariate_df = pd.DataFrame()\n",
    "phyhealth_df = pd.DataFrame()\n",
    "rsfc_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Function to merge a CSV file into a DataFrame on 'src_subject_id'\n",
    "def merge_csv(file_path, df):\n",
    "    new_df = pd.read_csv(file_path)\n",
    "    # Drop the 'eventname' column if it exists\n",
    "    if \"eventname\" in new_df.columns:\n",
    "        new_df = new_df.drop(columns=[\"eventname\"])\n",
    "    # Drop duplicate rows based on 'src_subject_id' in the new DataFrame\n",
    "    new_df = new_df.drop_duplicates(subset=\"src_subject_id\", keep=\"first\")\n",
    "    if df.empty:\n",
    "        return new_df\n",
    "    else:\n",
    "        # Merge the DataFrames on 'src_subject_id'\n",
    "        merged_df = pd.merge(df, new_df, on=\"src_subject_id\", how=\"outer\")\n",
    "        # Drop duplicate columns, keeping the first occurrence\n",
    "        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "        # Drop duplicate rows based on 'src_subject_id'\n",
    "        merged_df = merged_df.drop_duplicates(subset=\"src_subject_id\", keep=\"first\")\n",
    "        return merged_df\n",
    "\n",
    "\n",
    "# Iterate through files in the directory\n",
    "for filename in os.listdir(hisp_dir):\n",
    "    file_path = os.path.join(hisp_dir, filename)\n",
    "    if filename.startswith(\"ce\"):\n",
    "        sociocult_df = merge_csv(file_path, sociocult_df)\n",
    "    elif filename.startswith((\"abcd\", \"mri_y_adm\", \"mri_y_qc\")):\n",
    "        covariate_df = merge_csv(file_path, covariate_df)\n",
    "    elif filename.startswith(\"mri_y_rsfmr\"):\n",
    "        rsfc_df = merge_csv(file_path, rsfc_df)\n",
    "    elif filename.startswith((\"led\", \"mh\", \"ph\")):\n",
    "        phyhealth_df = merge_csv(file_path, phyhealth_df)\n",
    "\n",
    "# Display the first few rows of each DataFrame\n",
    "print(\"Sociocultural DataFrame:\")\n",
    "print(sociocult_df.head())\n",
    "\n",
    "print(\"\\nCovariate DataFrame:\")\n",
    "print(covariate_df.head())\n",
    "\n",
    "print(\"\\nPhysical Health DataFrame:\")\n",
    "print(phyhealth_df.head())\n",
    "\n",
    "print(\"\\nRSFC DataFrame:\")\n",
    "print(rsfc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving\n",
    "sociocult_save_path = os.path.join(deriv_dir, \"sociocult.csv\")\n",
    "covariate_save_path = os.path.join(deriv_dir, \"covariate.csv\")\n",
    "phyhealth_save_path = os.path.join(deriv_dir, \"phyhealth.csv\")\n",
    "rsfc_save_path = os.path.join(deriv_dir, \"rsfc.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "sociocult_df.to_csv(sociocult_save_path, index=False)\n",
    "covariate_df.to_csv(covariate_save_path, index=False)\n",
    "phyhealth_df.to_csv(phyhealth_save_path, index=False)\n",
    "rsfc_df.to_csv(rsfc_save_path, index=False)\n",
    "\n",
    "print(\"DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any NaN values\n",
    "sociocult_df_cleaned = sociocult_df.dropna()\n",
    "covariate_df_cleaned = covariate_df.dropna()\n",
    "\n",
    "print(sociocult_df_cleaned)\n",
    "print(covariate_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the intersection of subject IDs in both DataFrames\n",
    "common_ids = set(sociocult_df_cleaned[\"src_subject_id\"]).intersection(\n",
    "    set(covariate_df_cleaned[\"src_subject_id\"])\n",
    ")\n",
    "print(common_ids)\n",
    "\n",
    "# Count the number of common IDs\n",
    "print(\"Number of common subject IDs:\", len(common_ids))\n",
    "\n",
    "# Filter both DataFrames to keep only the rows with common subject IDs\n",
    "sociocult_df_final = sociocult_df_cleaned[\n",
    "    sociocult_df_cleaned[\"src_subject_id\"].isin(common_ids)\n",
    "]\n",
    "covariate_df_final = covariate_df_cleaned[\n",
    "    covariate_df_cleaned[\"src_subject_id\"].isin(common_ids)\n",
    "]\n",
    "rsfc_df_final = rsfc_df[rsfc_df[\"src_subject_id\"].isin(common_ids)]\n",
    "\n",
    "\n",
    "print(sociocult_df_final)\n",
    "print(covariate_df_final)\n",
    "print(rsfc_df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving\n",
    "sociocult_save_path = os.path.join(deriv_dir, \"sociocult.csv\")\n",
    "covariate_save_path = os.path.join(deriv_dir, \"covariate.csv\")\n",
    "rsfc_save_path = os.path.join(deriv_dir, \"rsfc.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "sociocult_df_final.to_csv(sociocult_save_path, index=False)\n",
    "covariate_df_final.to_csv(covariate_save_path, index=False)\n",
    "rsfc_df_final.to_csv(rsfc_save_path, index=False)\n",
    "\n",
    "print(\"DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data_summary(df, df_name):\n",
    "    # Calculate the number of non-NaN values for each column (except the first column)\n",
    "    non_nan_counts = df.iloc[:, 1:].notna().sum()\n",
    "    # Calculate the percentage of missing data for each column (except the first column)\n",
    "    missing_percentage = df.iloc[:, 1:].isna().mean() * 100\n",
    "\n",
    "    # Create the summary DataFrame\n",
    "    summary = pd.DataFrame(\n",
    "        {\n",
    "            \"Measure\": df.columns[1:],  # Exclude the first column\n",
    "            \"Number of Subjects\": non_nan_counts,  # Number of non-NaN values\n",
    "            \"Percentage Missing\": missing_percentage,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Determine if an asterisk should be added\n",
    "    summary[\"Above 5%\"] = summary[\"Percentage Missing\"].apply(\n",
    "        lambda x: \"*\" if x > 5 else \"\"\n",
    "    )\n",
    "\n",
    "    # Create a formatted string for the DataFrame summary\n",
    "    summary_str = f\"\\n{df_name} Missing Data Summary:\\n\"\n",
    "    summary_str += (\n",
    "        summary.to_string(\n",
    "            index=False,\n",
    "            columns=[\"Measure\", \"Number of Subjects\", \"Percentage Missing\", \"Above 5%\"],\n",
    "        )\n",
    "        + \"\\n\"\n",
    "    )\n",
    "\n",
    "    return summary, summary_str\n",
    "\n",
    "\n",
    "# Generate missing data summaries for each DataFrame\n",
    "sociocult_summary, sociocult_summary_str = missing_data_summary(\n",
    "    sociocult_df, \"Sociocultural DataFrame\"\n",
    ")\n",
    "covariate_summary, covariate_summary_str = missing_data_summary(\n",
    "    covariate_df, \"Covariate DataFrame\"\n",
    ")\n",
    "phyhealth_summary, phyhealth_summary_str = missing_data_summary(\n",
    "    phyhealth_df, \"Physical Health DataFrame\"\n",
    ")\n",
    "rsfc_summary, rsfc_summary_str = missing_data_summary(rsfc_df, \"RSFC DataFrame\")\n",
    "\n",
    "# Combine all summaries into one string\n",
    "all_summaries = (\n",
    "    sociocult_summary_str\n",
    "    + covariate_summary_str\n",
    "    + phyhealth_summary_str\n",
    "    + rsfc_summary_str\n",
    ")\n",
    "\n",
    "# Write the combined summaries to a text file\n",
    "with open(\"PLSC-missing_data.txt\", \"w\") as file:\n",
    "    file.write(all_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(color_codes=True)\n",
    "\n",
    "\n",
    "def plot_missing_data_summary(summary, df_name, deriv_dir):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    barplot = sns.barplot(\n",
    "        x=\"Percentage Missing\", y=\"Measure\", data=summary, palette=\"viridis\"\n",
    "    )\n",
    "    plt.title(f\"{df_name} Percentage of Missing Data\")\n",
    "    plt.xlabel(\"Percentage Missing (out of 589 subjects)\")\n",
    "    plt.ylabel(f\"ABCD {df_name} Measures\")\n",
    "    plt.xlim(0, 100)\n",
    "\n",
    "    # Add vertical lines\n",
    "    #plt.axvline(x=5, color=\"green\", linestyle=\"--\", label=\"5% Missing\")\n",
    "    #plt.axvline(x=10, color=\"red\", linestyle=\"--\", label=\"10% Missing\")\n",
    "\n",
    "    # Add custom legend with total number of subjects\n",
    "    plt.legend()\n",
    "\n",
    "    for index, row in summary.iterrows():\n",
    "        percentage_missing = format(row['Percentage Missing'], \".2f\")\n",
    "        text_color = 'red' if row['Percentage Missing'] > 5 else 'black'\n",
    "        barplot.annotate(f\"{percentage_missing}%\",\n",
    "                         xy=(row['Percentage Missing'], index),\n",
    "                         xytext=(20, 0),  # Move text slightly more to the right\n",
    "                         textcoords='offset points',\n",
    "                         ha='center', va='center', color=text_color)\n",
    "\n",
    "    # Save the figure\n",
    "    file_path = os.path.join(deriv_dir, f\"{df_name.replace(' ', '_')}_missing_data.png\")\n",
    "    plt.tight_layout()  # Ensure a tight layout with some padding\n",
    "    plt.savefig(file_path)\n",
    "    plt.close()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot and save the bar plots\n",
    "plot_missing_data_summary(sociocult_summary, \"Sociocultural\", deriv_dir)\n",
    "plot_missing_data_summary(covariate_summary, \"Covariate\", deriv_dir)\n",
    "plot_missing_data_summary(phyhealth_summary, \"Physical Health\", deriv_dir)\n",
    "# plot_missing_data_summary(rsfc_summary, \"RSFC DataFrame\", deriv_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the phyhealth_df to only include rows with src_subject_id in common_ids\n",
    "phyhealth_df_final = phyhealth_df[phyhealth_df[\"src_subject_id\"].isin(common_ids)]\n",
    "\n",
    "# Remove all rows with any NaN values\n",
    "phyhealth_df_final_cleaned = phyhealth_df_final.dropna()\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "print(\"DataFrame after removing rows with NaN values:\")\n",
    "print(phyhealth_df_final_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for saving\n",
    "phyhealth_save_path = os.path.join(deriv_dir, \"phyhealth_reg.csv\")\n",
    "\n",
    "# Save each DataFrame to a CSV file\n",
    "phyhealth_df_final_cleaned.to_csv(phyhealth_save_path, index=False)\n",
    "\n",
    "print(\"DataFrames have been saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you already have phyhealth_df_final\n",
    "common_ids = phyhealth_df_final_cleaned[\"src_subject_id\"].unique()\n",
    "\n",
    "# Filter other DataFrames using common_ids from phyhealth_df_final\n",
    "sociocult_df_reg = sociocult_df[sociocult_df[\"src_subject_id\"].isin(common_ids)]\n",
    "covariate_df_reg = covariate_df[covariate_df[\"src_subject_id\"].isin(common_ids)]\n",
    "rsfc_df_reg = rsfc_df[rsfc_df[\"src_subject_id\"].isin(common_ids)]\n",
    "\n",
    "# Now you have filtered versions of all DataFrames\n",
    "print(sociocult_df_reg, covariate_df_reg, rsfc_df_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrames as CSV files\n",
    "sociocult_df_final.to_csv(op.join(deriv_dir, \"sociocult_reg.csv\"), index=False)\n",
    "covariate_df_final.to_csv(op.join(deriv_dir, \"covariate_reg.csv\"), index=False)\n",
    "rsfc_df_final.to_csv(op.join(deriv_dir, \"rsfc_reg.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
